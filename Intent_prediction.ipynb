{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(columns=['text','Intent'])\n",
    "filenames = list()\n",
    "intervention_subgraphs = list()\n",
    "for directory, subdirectories, files in os.walk('C:/Users/Tamir/Desktop/HSE_Project/excel_files/'):\n",
    "    for file in files:\n",
    "        filenames.append(os.path.join(directory,file))\n",
    "for filename in filenames:\n",
    "    if (filename[len(filename) - 22] == '~'):\n",
    "        continue\n",
    "    filename = filename.replace(\" \", \" \")\n",
    "    post_df = pd.read_excel(filename)\n",
    "    \n",
    "        #fix to add columns if they are not in the xlsx file\n",
    "    if 'attachments' not in post_df.columns:\n",
    "        post_df['attachments'] = ''\n",
    "    if 'reply_to_comment' not in post_df.columns:\n",
    "        post_df['reply_to_comment'] = 0\n",
    "    if 'reply_to_user' not in post_df.columns:\n",
    "        post_df['reply_to_user'] = 0\n",
    "        \n",
    "    flex_df = post_df.drop(columns=['attachments','date','from_id','id','reply_to_comment','reply_to_user','Content','Intervention'])\n",
    "    data_df = data_df.append(flex_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change in 3158\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Removes stop words from the given text\n",
    "    :param text: string with the text\n",
    "    :param remove_stopwords: flag to enable removal of stop words\n",
    "    :return: set of tokens without any stop words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tags = list()\n",
    "    \n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('russian'))\n",
    "        tokens = [w for w in tokens if w not in stops]\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = morph.parse(tokens[i])[0].normal_form\n",
    "        t = morph.parse(tokens[i])[0]\n",
    "        tokens[i] = tokens[i] + '_' + str(t.tag.POS)\n",
    "    return tokens\n",
    "\n",
    "delete_index = list()\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    if ( pd.isnull(data_df['text'].loc[index]) or pd.isnull(data_df['Intent'].loc[index]) \n",
    "              or (data_df['Intent'].loc[index] == ' -' ) ):\n",
    "        delete_index.append(index)        \n",
    "    else:\n",
    "        data_df['text'].loc[index] = clean_text(data_df['text'].loc[index])\n",
    "\n",
    "data_df = data_df.drop(index = delete_index)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    data_df['Intent'].loc[index] = data_df['Intent'].loc[index][0]\n",
    "    if(data_df['Intent'].loc[index] == 'E'):\n",
    "        print('change in ' + str(index))\n",
    "        data_df['Intent'].loc[index] = 'Е'\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#pymorphy_tags={\n",
    "#        +'NOUN',  # имя существительное\n",
    "#        +'ADJF',  # имя прилагательное (полное)\n",
    "#        +'ADJS',  # имя прилагательное (краткое)\n",
    "#        +'VERB',  # глагол (личная форма)\n",
    "#        +'INFN',  # глагол (инфинитив)\n",
    "#        +'PRTF',  # причастие (полное)\n",
    "#        +'PRTS',  # причастие (краткое)\n",
    "#        +'GRND',  # деепричастие\n",
    "#        +'NUMR',  # числительное\n",
    "#        +'ADVB',  # наречие\n",
    "#        +'NPRO',  # местоимение-существительное\n",
    "#        +'PRED',  # предикатив\n",
    "#        +'PREP',  # предлог\n",
    "#        +'CONJ',  # союз\n",
    "#        +'PRCL',  # частица\n",
    "#        +'INTJ',  # междометие\n",
    "#}\n",
    "#universal_tags={\n",
    "#    +#ADJ: adjective\n",
    "#    +#ADP: adposition\n",
    "#    +#ADV: adverb\n",
    "#    +#CCONJ: coordinating conjunction\n",
    "#    +#DET: determiner\n",
    "#    +#INTJ: interjection\n",
    "#    +#NOUN: noun\n",
    "#    +#NUM: numeral\n",
    "#    +#PART: particle\n",
    "#    +#PRON: pronoun\n",
    "#    +#VERB: verb\n",
    "#}\n",
    "\n",
    "#Dict for tranformation pymorphy2 tags into universal POS tags\n",
    "\n",
    "transform_dict = {'NOUN':'NOUN',\n",
    "                  'ADJF':'ADJ',\n",
    "                  'ADJS':'ADJ',\n",
    "                  'PREP':'ADP',\n",
    "                  'ADVB':'ADV',\n",
    "                  'VERB':'VERB',\n",
    "                  'INFN':'VERB',\n",
    "                  'CONJ':'CCONJ',\n",
    "                  'NPRO':'PRON',\n",
    "                  'INTJ':'INTJ',\n",
    "                  'PRCL':'PART',\n",
    "                  'NUMR':'NUM',\n",
    "                  'PRTF':'ADJ',\n",
    "                  'PRTS':'ADJ',\n",
    "                  'GRND':'ADJ',\n",
    "                  'PRED':'ADJ'}\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    delete_list = list()\n",
    "    #print('new comment')\n",
    "    comment = data_df['text'].loc[index]\n",
    "    for i in range(len(comment) - 1):\n",
    "        #print('word: ' + comment[i])\n",
    "        tag = comment[i].split('_')[len(comment[i].split('_'))-1]\n",
    "\n",
    "        #print('old tag: ' + tag)\n",
    "        if (tag == 'None'):\n",
    "            delete_list.append(comment[i])\n",
    "        else:\n",
    "            tag = transform_dict.get(tag)\n",
    "            #print('new tag: ' + tag)\n",
    "            comment[i] = comment[i].split('_')[0] + '_' + tag\n",
    "    for delete_value in delete_list:\n",
    "        comment.remove(delete_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "from gensim.models.wrappers import FastText\n",
    "import gensim.downloader as api\n",
    "\n",
    "#FastText.load_model('cc.ru.300')\n",
    "model = api.load(\"word2vec-ruscorpora-300\")\n",
    "\n",
    "#w2v_file = codecs.open('cc.ru.300.bin.gz', encoding='utf-8')\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)  # or binary=False if the model is not compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the words in data after preprocessing: 47792\n",
      "Words in model vacabulary: 36163\n"
     ]
    }
   ],
   "source": [
    "def create_average_vec(doc, vec_dim, ft_model):\n",
    "    average = np.zeros((vec_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    for word in doc:\n",
    "        if word in ft_model.wv.vocab:\n",
    "            average = np.add(average, ft_model[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average\n",
    "\n",
    "word_counter = 0\n",
    "model_counter = 0\n",
    "for index,row in data_df.iterrows():\n",
    "    comment = data_df['text'].loc[index]\n",
    "    for word in comment:\n",
    "        word_counter += 1\n",
    "        if (word in model.vocab):\n",
    "            model_counter += 1\n",
    "print('All the words in data after preprocessing: ' + str(word_counter))\n",
    "print('Words in model vacabulary: ' + str(model_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vector_data = list()\n",
    "\n",
    "\n",
    "for index,row in data_df.iterrows(): \n",
    "    comment = data_df['text'].loc[index]\n",
    "    comment_vector = np.zeros((300,), dtype='float32')\n",
    "    words = 0\n",
    "    for word in comment:\n",
    "        if (word in model.vocab):\n",
    "            comment_vector = np.add(comment_vector,model[word])\n",
    "            words += 1\n",
    "    if (words != 0):\n",
    "        comment_vector = np.divide(comment_vector,words)\n",
    "    vector_data.append(comment_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 25)                3225      \n",
      "=================================================================\n",
      "Total params: 113,177\n",
      "Trainable params: 113,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2928/2928 [==============================] - 1s 188us/step - loss: 2.8001 - acc: 0.1800\n",
      "Epoch 2/100\n",
      "2928/2928 [==============================] - 0s 90us/step - loss: 2.6726 - acc: 0.2008\n",
      "Epoch 3/100\n",
      "2928/2928 [==============================] - 0s 79us/step - loss: 2.5998 - acc: 0.2254\n",
      "Epoch 4/100\n",
      "2928/2928 [==============================] - 0s 77us/step - loss: 2.5234 - acc: 0.2572\n",
      "Epoch 5/100\n",
      "2928/2928 [==============================] - 0s 73us/step - loss: 2.4486 - acc: 0.2797\n",
      "Epoch 6/100\n",
      "2928/2928 [==============================] - 0s 77us/step - loss: 2.3820 - acc: 0.2964\n",
      "Epoch 7/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 2.3059 - acc: 0.3091\n",
      "Epoch 8/100\n",
      "2928/2928 [==============================] - 0s 64us/step - loss: 2.2451 - acc: 0.3309\n",
      "Epoch 9/100\n",
      "2928/2928 [==============================] - 0s 64us/step - loss: 2.1831 - acc: 0.3494\n",
      "Epoch 10/100\n",
      "2928/2928 [==============================] - 0s 81us/step - loss: 2.1171 - acc: 0.3651\n",
      "Epoch 11/100\n",
      "2928/2928 [==============================] - 0s 64us/step - loss: 2.0558 - acc: 0.3791\n",
      "Epoch 12/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 1.9877 - acc: 0.4010\n",
      "Epoch 13/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 1.9280 - acc: 0.4228\n",
      "Epoch 14/100\n",
      "2928/2928 [==============================] - 0s 80us/step - loss: 1.8733 - acc: 0.4317\n",
      "Epoch 15/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 1.8097 - acc: 0.4549\n",
      "Epoch 16/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 1.7517 - acc: 0.4665\n",
      "Epoch 17/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 1.6909 - acc: 0.4935\n",
      "Epoch 18/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 1.6314 - acc: 0.5007\n",
      "Epoch 19/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 1.5777 - acc: 0.5294\n",
      "Epoch 20/100\n",
      "2928/2928 [==============================] - 0s 84us/step - loss: 1.5173 - acc: 0.5519\n",
      "Epoch 21/100\n",
      "2928/2928 [==============================] - 0s 76us/step - loss: 1.4663 - acc: 0.5680\n",
      "Epoch 22/100\n",
      "2928/2928 [==============================] - 0s 74us/step - loss: 1.4066 - acc: 0.5847\n",
      "Epoch 23/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 1.3559 - acc: 0.6055\n",
      "Epoch 24/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 1.3006 - acc: 0.6243\n",
      "Epoch 25/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 1.2496 - acc: 0.6373\n",
      "Epoch 26/100\n",
      "2928/2928 [==============================] - 0s 80us/step - loss: 1.1991 - acc: 0.6513\n",
      "Epoch 27/100\n",
      "2928/2928 [==============================] - 0s 72us/step - loss: 1.1467 - acc: 0.6790\n",
      "Epoch 28/100\n",
      "2928/2928 [==============================] - 0s 62us/step - loss: 1.1028 - acc: 0.6858\n",
      "Epoch 29/100\n",
      "2928/2928 [==============================] - 0s 73us/step - loss: 1.0497 - acc: 0.7087\n",
      "Epoch 30/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 1.0007 - acc: 0.7230\n",
      "Epoch 31/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.9598 - acc: 0.7319\n",
      "Epoch 32/100\n",
      "2928/2928 [==============================] - 0s 71us/step - loss: 0.9049 - acc: 0.7534\n",
      "Epoch 33/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.8690 - acc: 0.7640\n",
      "Epoch 34/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.8279 - acc: 0.7787\n",
      "Epoch 35/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.7845 - acc: 0.7958\n",
      "Epoch 36/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.7418 - acc: 0.8009\n",
      "Epoch 37/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.7130 - acc: 0.8087\n",
      "Epoch 38/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.6830 - acc: 0.8200\n",
      "Epoch 39/100\n",
      "2928/2928 [==============================] - 0s 73us/step - loss: 0.6431 - acc: 0.8323\n",
      "Epoch 40/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.6065 - acc: 0.8405\n",
      "Epoch 41/100\n",
      "2928/2928 [==============================] - 0s 72us/step - loss: 0.5801 - acc: 0.8487\n",
      "Epoch 42/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 0.5498 - acc: 0.8620\n",
      "Epoch 43/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.5233 - acc: 0.8726\n",
      "Epoch 44/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.4994 - acc: 0.8764\n",
      "Epoch 45/100\n",
      "2928/2928 [==============================] - 0s 78us/step - loss: 0.4758 - acc: 0.8835\n",
      "Epoch 46/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 0.4446 - acc: 0.8941\n",
      "Epoch 47/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.4275 - acc: 0.8934\n",
      "Epoch 48/100\n",
      "2928/2928 [==============================] - 0s 63us/step - loss: 0.4023 - acc: 0.9057\n",
      "Epoch 49/100\n",
      "2928/2928 [==============================] - 0s 86us/step - loss: 0.3833 - acc: 0.9122\n",
      "Epoch 50/100\n",
      "2928/2928 [==============================] - 0s 79us/step - loss: 0.3647 - acc: 0.9156\n",
      "Epoch 51/100\n",
      "2928/2928 [==============================] - 0s 76us/step - loss: 0.3584 - acc: 0.9173\n",
      "Epoch 52/100\n",
      "2928/2928 [==============================] - 0s 63us/step - loss: 0.3272 - acc: 0.9296\n",
      "Epoch 53/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.3180 - acc: 0.9273\n",
      "Epoch 54/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.3109 - acc: 0.9314\n",
      "Epoch 55/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.2912 - acc: 0.9382\n",
      "Epoch 56/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.2849 - acc: 0.9365\n",
      "Epoch 57/100\n",
      "2928/2928 [==============================] - 0s 79us/step - loss: 0.2708 - acc: 0.9426\n",
      "Epoch 58/100\n",
      "2928/2928 [==============================] - 0s 70us/step - loss: 0.2620 - acc: 0.9416\n",
      "Epoch 59/100\n",
      "2928/2928 [==============================] - 0s 78us/step - loss: 0.2501 - acc: 0.9443\n",
      "Epoch 60/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.2431 - acc: 0.9474\n",
      "Epoch 61/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.2362 - acc: 0.9454\n",
      "Epoch 62/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.2287 - acc: 0.9474\n",
      "Epoch 63/100\n",
      "2928/2928 [==============================] - ETA: 0s - loss: 0.2238 - acc: 0.948 - 0s 68us/step - loss: 0.2267 - acc: 0.9484\n",
      "Epoch 64/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.2202 - acc: 0.9515\n",
      "Epoch 65/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.2179 - acc: 0.9495\n",
      "Epoch 66/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.2098 - acc: 0.9515\n",
      "Epoch 67/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.2041 - acc: 0.9515\n",
      "Epoch 68/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.2050 - acc: 0.9525\n",
      "Epoch 69/100\n",
      "2928/2928 [==============================] - 0s 70us/step - loss: 0.2007 - acc: 0.9501\n",
      "Epoch 70/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.2035 - acc: 0.9532\n",
      "Epoch 71/100\n",
      "2928/2928 [==============================] - 0s 71us/step - loss: 0.1923 - acc: 0.9546\n",
      "Epoch 72/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1948 - acc: 0.9518\n",
      "Epoch 73/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1964 - acc: 0.9515\n",
      "Epoch 74/100\n",
      "2928/2928 [==============================] - 0s 72us/step - loss: 0.1860 - acc: 0.9546\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.1821 - acc: 0.9529\n",
      "Epoch 76/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1913 - acc: 0.9539\n",
      "Epoch 77/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1911 - acc: 0.9505\n",
      "Epoch 78/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1836 - acc: 0.9529\n",
      "Epoch 79/100\n",
      "2928/2928 [==============================] - 0s 73us/step - loss: 0.1794 - acc: 0.9536\n",
      "Epoch 80/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.1819 - acc: 0.9525\n",
      "Epoch 81/100\n",
      "2928/2928 [==============================] - 0s 68us/step - loss: 0.1900 - acc: 0.9518\n",
      "Epoch 82/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1791 - acc: 0.9532\n",
      "Epoch 83/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.1754 - acc: 0.9536\n",
      "Epoch 84/100\n",
      "2928/2928 [==============================] - 0s 71us/step - loss: 0.1857 - acc: 0.9477\n",
      "Epoch 85/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1732 - acc: 0.9539\n",
      "Epoch 86/100\n",
      "2928/2928 [==============================] - 0s 64us/step - loss: 0.1711 - acc: 0.9529\n",
      "Epoch 87/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.1769 - acc: 0.9515\n",
      "Epoch 88/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.1731 - acc: 0.9563\n",
      "Epoch 89/100\n",
      "2928/2928 [==============================] - 0s 73us/step - loss: 0.1743 - acc: 0.9566\n",
      "Epoch 90/100\n",
      "2928/2928 [==============================] - 0s 74us/step - loss: 0.1703 - acc: 0.9539\n",
      "Epoch 91/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 0.1706 - acc: 0.9518\n",
      "Epoch 92/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1720 - acc: 0.9525\n",
      "Epoch 93/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.1684 - acc: 0.9556\n",
      "Epoch 94/100\n",
      "2928/2928 [==============================] - 0s 66us/step - loss: 0.1626 - acc: 0.9549\n",
      "Epoch 95/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.1679 - acc: 0.9542\n",
      "Epoch 96/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.1669 - acc: 0.9518\n",
      "Epoch 97/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1640 - acc: 0.9553\n",
      "Epoch 98/100\n",
      "2928/2928 [==============================] - 0s 65us/step - loss: 0.1712 - acc: 0.9512\n",
      "Epoch 99/100\n",
      "2928/2928 [==============================] - 0s 69us/step - loss: 0.1606 - acc: 0.9553\n",
      "Epoch 100/100\n",
      "2928/2928 [==============================] - 0s 67us/step - loss: 0.1708 - acc: 0.9525\n",
      "[5.057857333231428, 0.4038199177786891]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "label_list = list()\n",
    "for intent in data_df['Intent']:\n",
    "    label_list.append(intent)\n",
    "\n",
    "# 4. Load pre-shuffled MNIST data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_data, label_list, test_size=0.2, random_state = 42, stratify = label_list)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train) \n",
    "#y_test = np.array(y_test)\n",
    "\n",
    "# 5. Preprocess input data\n",
    "#X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "#Y_train = np_utils.to_categorical(y_train, 25)\n",
    "#Y_test = np_utils.to_categorical(y_test, 25)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(label_list)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "#model.add(Conv1D(128, 5, activation='relu',input_shape = (1,300)))\n",
    "#model.add(MaxPooling1D(5))\n",
    "#model.add(Conv1D(128, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(5))\n",
    "#model.add(Conv1D(128, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(35))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "model.add(Dense(256,activation='relu',input_dim=300))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(25,activation='softmax'))\n",
    "\n",
    "model.summary()         \n",
    "#model.add(Conv1D(128, 2, activation='relu',input_dim = 300, padding = 'same'))\n",
    "#model.add(MaxPooling1D(1))\n",
    "#model.add(Conv1D(128, 2, activation='relu'))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dense(25, activation='softmax'))\n",
    "#model.summary()\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#                      optimizer='rmsprop',\n",
    "#                      metrics=['acc'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "          \n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, y_train, epochs=100, verbose=1)\n",
    " \n",
    "# 10. Evaluate model on test data\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3661\n",
      "3661\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test)+len(X_train))\n",
    "print(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Щ\n"
     ]
    }
   ],
   "source": [
    "print(data_df['Intent'][35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ага\n",
      "ага_INTJ\n",
      "держи\n",
      "держать_INFN\n",
      "карман\n",
      "карман_NOUN\n",
      "шире\n",
      "шире_NOUN\n",
      "['ага_INTJ', 'держать_INFN', 'карман_NOUN', 'шире_NOUN']\n",
      "ага\n",
      "ага_INTJ\n",
      "держи\n",
      "держать_INFN\n",
      "карман\n",
      "карман_NOUN\n",
      "шире\n",
      "шире_NOUN\n",
      "['ага', 'держи', 'карман', 'шире']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['ага','держи','карман','шире']\n",
    "for i in range(len(tokens)):\n",
    "    print(tokens[i])\n",
    "    tokens[i] = morph.parse(tokens[i])[0].normal_form\n",
    "    t = morph.parse(tokens[i])[0]\n",
    "    tokens[i] = tokens[i] + '_' + str(t.tag.POS)\n",
    "    print(tokens[i])\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = ['ага','держи','карман','шире']\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    token = morph.parse(token)[0].normal_form\n",
    "    t = morph.parse(token)[0]\n",
    "    token = token+'_'+str(t.tag.POS)\n",
    "    print(token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "word = 'дома'\n",
    "word = morph.parse(word)[0].normal_form\n",
    "t = morph.parse(word)[0]\n",
    "word = word+'_'+str(t.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.21383590e-02,  9.23040230e-03,  4.93700095e-02,  3.90502401e-02,\n",
       "       -6.40397668e-02, -5.30002415e-02,  4.67836894e-02, -4.85614082e-03,\n",
       "        4.02406836e-03, -9.79618821e-03,  2.84713488e-02,  9.16718785e-03,\n",
       "        1.10953590e-02, -3.90402414e-02,  1.21535128e-02, -8.35934207e-02,\n",
       "        7.45098069e-02,  1.62404403e-03,  1.74726788e-02,  3.05266287e-02,\n",
       "        5.99543266e-02, -2.14239806e-02, -1.49271898e-02, -1.01301707e-02,\n",
       "        1.79782417e-02, -2.90299635e-02,  3.24524790e-02, -2.79778410e-02,\n",
       "       -2.37032931e-04, -5.76303340e-02,  4.24205773e-02,  6.51675323e-03,\n",
       "        6.55684322e-02, -5.64103164e-02,  7.23182922e-03,  4.41771112e-02,\n",
       "        4.30161357e-02, -1.89267937e-02, -4.42381762e-02,  2.29172464e-02,\n",
       "        1.28449202e-02, -1.86000392e-02,  1.11480644e-02, -3.29818800e-02,\n",
       "       -5.81772774e-02,  1.06284674e-03,  7.04645813e-02,  2.48348117e-02,\n",
       "        7.25177452e-02, -2.24636495e-02,  9.71535686e-03,  4.66136783e-02,\n",
       "       -1.86466146e-02, -8.38969648e-02,  3.59005816e-02, -1.77096259e-02,\n",
       "       -5.18482141e-02, -1.99117884e-03, -4.88198400e-02, -5.86908916e-03,\n",
       "        3.26357484e-02,  4.10769135e-02,  3.64902280e-02, -1.72313545e-02,\n",
       "        2.56396015e-03,  2.63759028e-02, -5.55515289e-02,  5.70319779e-02,\n",
       "       -6.21985318e-03,  1.14440881e-01, -2.67217103e-02,  1.49783632e-02,\n",
       "       -1.47750117e-02, -8.68485793e-02, -1.98957808e-02,  6.38343468e-02,\n",
       "        6.40812144e-03, -9.08489749e-02,  4.91447106e-04, -1.24332367e-03,\n",
       "       -7.01033473e-02,  2.25459766e-02,  2.56275404e-02, -2.36325264e-02,\n",
       "       -5.81569374e-02, -1.28761372e-02, -2.54866276e-02, -2.65129860e-02,\n",
       "        1.25710368e-02,  4.22222503e-02, -4.41931784e-02,  1.21008083e-02,\n",
       "       -1.89381968e-02, -5.85941970e-02,  2.97973454e-02,  3.36655118e-02,\n",
       "       -6.20474368e-02, -2.53928006e-02, -3.63629870e-02, -1.53579349e-02,\n",
       "       -7.84479454e-03,  1.89516377e-02, -8.31698533e-03,  4.32747649e-03,\n",
       "        2.56717089e-03,  4.30661924e-02,  1.78529285e-02, -2.72935051e-02,\n",
       "        1.48287667e-02,  1.50117362e-02,  3.29466015e-02,  3.81167382e-02,\n",
       "        3.33472230e-02,  2.73478348e-02,  5.06543852e-02, -3.35998721e-02,\n",
       "       -3.27158757e-02,  6.13155924e-02, -6.98935464e-02, -2.52783541e-02,\n",
       "        7.77321309e-02,  1.78909823e-02, -4.75053489e-02, -8.70597083e-03,\n",
       "        6.40653679e-03,  2.58678067e-02, -4.35886197e-02,  2.00348850e-02,\n",
       "        2.20343620e-02,  8.33982043e-03, -7.73087749e-03,  7.00825593e-03,\n",
       "        1.60525460e-02,  6.69912726e-04,  4.94994037e-02, -5.47502339e-02,\n",
       "        8.04615114e-03,  4.51828130e-02,  1.86610203e-02,  1.68955550e-02,\n",
       "        6.04482703e-02, -6.82015298e-03, -7.41234375e-03, -3.92200947e-02,\n",
       "       -4.53835875e-02, -5.57332449e-02, -5.17362393e-02,  2.78323647e-02,\n",
       "       -9.97470785e-03, -3.43489391e-03,  3.46429646e-02, -4.76452671e-02,\n",
       "       -2.05069948e-02,  4.12932150e-02,  3.59748639e-02, -1.66118201e-02,\n",
       "        4.62624878e-02,  2.49800086e-02, -1.18066529e-02,  2.32257079e-02,\n",
       "        9.13205277e-03, -3.48931886e-02,  2.38917861e-02, -4.62409221e-02,\n",
       "       -5.32183610e-02, -6.69679865e-02,  1.57458615e-02,  3.67742777e-02,\n",
       "       -1.00029325e-02, -1.29664317e-03, -7.45882699e-03,  4.14919248e-03,\n",
       "        4.55988944e-03,  4.16733362e-02,  3.09250820e-02,  4.86152340e-03,\n",
       "       -8.98102392e-03, -4.79127355e-02, -4.82205534e-03,  4.19262350e-02,\n",
       "       -4.80587184e-02,  5.81463128e-02, -4.14445512e-02,  1.99350640e-02,\n",
       "       -3.06133628e-02, -7.62157068e-02, -4.41019647e-02, -6.46387637e-02,\n",
       "       -4.54791598e-02, -1.46843717e-02,  7.10219517e-03, -3.56017239e-02,\n",
       "       -7.88732097e-02, -3.22604105e-02, -5.98612102e-03,  1.02355607e-01,\n",
       "       -2.73691844e-02, -6.76873326e-02,  6.03678487e-02, -3.48345675e-02,\n",
       "       -6.48623928e-02, -7.87829980e-04, -1.08003393e-02, -7.52843991e-02,\n",
       "       -4.32252102e-02,  6.91997865e-03, -3.59397568e-02, -5.05983829e-02,\n",
       "        3.11378539e-02,  8.72637928e-02,  1.06565468e-02, -2.63223629e-02,\n",
       "       -6.33145049e-02,  2.04084888e-02,  4.99686450e-02,  9.31018963e-02,\n",
       "        8.65267590e-03,  6.15947805e-02, -4.57874686e-02,  7.35372817e-03,\n",
       "       -5.95156513e-02,  3.29111479e-02,  3.28985043e-02, -1.81433803e-03,\n",
       "        9.14859679e-03, -2.88655777e-02,  6.13371320e-02,  4.83609885e-02,\n",
       "        1.26392059e-02,  6.78882152e-02,  6.37787655e-02, -2.92812642e-02,\n",
       "       -6.83465898e-02,  7.45775327e-02, -1.15184672e-01, -2.68098270e-03,\n",
       "        1.14820497e-02,  5.41222245e-02, -1.08468486e-02, -6.25540987e-02,\n",
       "        6.39198199e-02,  5.64951785e-02, -7.15947151e-03, -1.71098635e-02,\n",
       "       -7.89980292e-02,  3.70329022e-02,  2.77316421e-02, -2.40728129e-02,\n",
       "       -5.44096902e-03,  1.15522370e-02,  6.07626094e-03, -3.93910930e-02,\n",
       "       -5.40286265e-02,  1.27665931e-02,  4.46829945e-02, -1.80318148e-03,\n",
       "        5.12746274e-02,  6.41936297e-03,  1.42175527e-02, -1.08263791e-02,\n",
       "       -9.32721328e-03,  9.44173988e-03, -8.84069130e-02,  2.75910255e-02,\n",
       "       -2.00370271e-02,  1.26091987e-02, -4.05395888e-02, -3.64367701e-02,\n",
       "       -2.51068473e-02, -1.96218397e-02,  2.28716880e-02,  2.54994612e-02,\n",
       "       -1.41113186e-02,  1.42419040e-02,  1.38294408e-02,  2.93593761e-02,\n",
       "       -8.21749959e-03,  2.56328434e-02, -4.41145115e-02,  7.25838542e-03,\n",
       "       -7.90673420e-02, -4.11110073e-02, -6.23700544e-05,  2.84980237e-02,\n",
       "        3.91152315e-02,  3.16206068e-02, -4.50731330e-02, -1.10713067e-02,\n",
       "       -2.09797304e-02,  4.22026850e-02, -3.40153533e-03,  4.29721214e-02,\n",
       "       -9.61882249e-03,  3.95904444e-02, -6.51099309e-02, -2.44326498e-02,\n",
       "       -2.84385215e-02, -2.92446818e-02, -2.64154952e-02, -1.61072146e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ц', 'Н', 'Ш', 'Ж', 'И', 'Ч', 'Т', 'Щ', 'А', 'Г', 'У', 'Х', 'Б',\n",
       "       'Д', 'Л', 'В', 'Е', 'П', 'К', 'С', 'М', 'Ф', 'З', 'Р', 'О'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Tamir\\AppData\\Local\\pip\\Cache\\wheels\\9b\\04\\dd\\7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: dawg-python, docopt, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n",
      "distributed 1.20.2 requires msgpack-python, which is not installed.\n",
      "tensorflow 1.11.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.4.3 which is incompatible.\n",
      "pyowm 2.8.0 has requirement requests<2.19,>=2.18.2, but you'll have requests 2.19.1 which is incompatible.\n",
      "copydoc 1.0.9 has requirement beautifulsoup4==4.4.1, but you'll have beautifulsoup4 4.6.3 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
