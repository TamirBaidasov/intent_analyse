{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(columns=['text','Intent'])\n",
    "filenames = list()\n",
    "intervention_subgraphs = list()\n",
    "for directory, subdirectories, files in os.walk('C:/Users/Tamir/Desktop/HSE_Project/excel_files/'):\n",
    "    for file in files:\n",
    "        filenames.append(os.path.join(directory,file))\n",
    "for filename in filenames:\n",
    "    if (filename[len(filename) - 22] == '~'):\n",
    "        continue\n",
    "    filename = filename.replace(\" \", \" \")\n",
    "    post_df = pd.read_excel(filename)\n",
    "    \n",
    "        #fix to add columns if they are not in the xlsx file\n",
    "    if 'attachments' not in post_df.columns:\n",
    "        post_df['attachments'] = ''\n",
    "    if 'reply_to_comment' not in post_df.columns:\n",
    "        post_df['reply_to_comment'] = 0\n",
    "    if 'reply_to_user' not in post_df.columns:\n",
    "        post_df['reply_to_user'] = 0\n",
    "        \n",
    "    flex_df = post_df.drop(columns=['attachments','date','from_id','id','reply_to_comment','reply_to_user','Content','Intervention'])\n",
    "    data_df = data_df.append(flex_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change in 3158\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Removes stop words from the given text\n",
    "    :param text: string with the text\n",
    "    :param remove_stopwords: flag to enable removal of stop words\n",
    "    :return: set of tokens without any stop words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tags = list()\n",
    "    \n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('russian'))\n",
    "        tokens = [w for w in tokens if w not in stops]\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = morph.parse(tokens[i])[0].normal_form\n",
    "        t = morph.parse(tokens[i])[0]\n",
    "        tokens[i] = tokens[i] + '_' + str(t.tag.POS)\n",
    "    return tokens\n",
    "\n",
    "delete_index = list()\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    if ( pd.isnull(data_df['text'].loc[index]) or pd.isnull(data_df['Intent'].loc[index]) \n",
    "              or (data_df['Intent'].loc[index] == ' -' ) ):\n",
    "        delete_index.append(index)        \n",
    "    else:\n",
    "        data_df['text'].loc[index] = clean_text(data_df['text'].loc[index])\n",
    "\n",
    "data_df = data_df.drop(index = delete_index)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    data_df['Intent'].loc[index] = data_df['Intent'].loc[index][0]\n",
    "    if(data_df['Intent'].loc[index] == 'E'):\n",
    "        print('change in ' + str(index))\n",
    "        data_df['Intent'].loc[index] = 'Е'\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-e3f7234a2096>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m#print('new tag: ' + tag)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mcomment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdelete_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdelete_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "#import json\n",
    "#pymorphy_tags={\n",
    "#        +'NOUN',  # имя существительное\n",
    "#        +'ADJF',  # имя прилагательное (полное)\n",
    "#        +'ADJS',  # имя прилагательное (краткое)\n",
    "#        +'VERB',  # глагол (личная форма)\n",
    "#        +'INFN',  # глагол (инфинитив)\n",
    "#        +'PRTF',  # причастие (полное)\n",
    "#        +'PRTS',  # причастие (краткое)\n",
    "#        +'GRND',  # деепричастие\n",
    "#        +'NUMR',  # числительное\n",
    "#        +'ADVB',  # наречие\n",
    "#        +'NPRO',  # местоимение-существительное\n",
    "#        +'PRED',  # предикатив\n",
    "#        +'PREP',  # предлог\n",
    "#        +'CONJ',  # союз\n",
    "#        +'PRCL',  # частица\n",
    "#        +'INTJ',  # междометие\n",
    "#}\n",
    "#universal_tags={\n",
    "#    +#ADJ: adjective\n",
    "#    +#ADP: adposition\n",
    "#    +#ADV: adverb\n",
    "#    +#CCONJ: coordinating conjunction\n",
    "#    +#DET: determiner\n",
    "#    +#INTJ: interjection\n",
    "#    +#NOUN: noun\n",
    "#    +#NUM: numeral\n",
    "#    +#PART: particle\n",
    "#    +#PRON: pronoun\n",
    "#    +#VERB: verb\n",
    "#}\n",
    "\n",
    "#Dict for tranformation pymorphy2 tags into universal POS tags\n",
    "\n",
    "transform_dict = {'NOUN':'NOUN',\n",
    "                  'ADJF':'ADJ',\n",
    "                  'ADJS':'ADJ',\n",
    "                  'PREP':'ADP',\n",
    "                  'ADVB':'ADV',\n",
    "                  'VERB':'VERB',\n",
    "                  'INFN':'VERB',\n",
    "                  'CONJ':'CCONJ',\n",
    "                  'NPRO':'PRON',\n",
    "                  'INTJ':'INTJ',\n",
    "                  'PRCL':'PART',\n",
    "                  'NUMR':'NUM',\n",
    "                  'PRTF':'ADJ',\n",
    "                  'PRTS':'ADJ',\n",
    "                  'GRND':'ADJ',\n",
    "                  'PRED':'ADJ'}\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    delete_list = list()\n",
    "    #print('new comment')\n",
    "    comment = data_df['text'].loc[index]\n",
    "    for i in range(len(comment) - 1):\n",
    "        #print('word: ' + comment[i])\n",
    "        tag = comment[i].split('_')[len(comment[i].split('_'))-1]\n",
    "\n",
    "        #print('old tag: ' + tag)\n",
    "        if (tag == 'None'):\n",
    "            delete_list.append(comment[i])\n",
    "        else:\n",
    "            tag = transform_dict.get(tag)\n",
    "            #print('new tag: ' + tag)\n",
    "            comment[i] = comment[i].split('_')[0] + '_' + tag\n",
    "    for delete_value in delete_list:\n",
    "        comment.remove(delete_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the words in data after preprocessing: 47792\n",
      "Words in model vacabulary: 36163\n"
     ]
    }
   ],
   "source": [
    "def create_average_vec(doc, vec_dim, ft_model):\n",
    "    average = np.zeros((vec_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    for word in doc:\n",
    "        if word in ft_model.wv.vocab:\n",
    "            average = np.add(average, ft_model[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average\n",
    "\n",
    "word_counter = 0\n",
    "model_counter = 0\n",
    "for index,row in data_df.iterrows():\n",
    "    comment = data_df['text'].loc[index]\n",
    "    for word in comment:\n",
    "        word_counter += 1\n",
    "        if (word in model.vocab):\n",
    "            model_counter += 1\n",
    "print('All the words in data after preprocessing: ' + str(word_counter))\n",
    "print('Words in model vacabulary: ' + str(model_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vector_data = list()\n",
    "\n",
    "\n",
    "for index,row in data_df.iterrows(): \n",
    "    comment = data_df['text'].loc[index]\n",
    "    comment_vector = np.zeros((300,), dtype='float32')\n",
    "    words = 0\n",
    "    for word in comment:\n",
    "        if (word in model.vocab):\n",
    "            comment_vector = np.add(comment_vector,model[word])\n",
    "            words += 1\n",
    "    if (words != 0):\n",
    "        comment_vector = np.divide(comment_vector,words)\n",
    "    vector_data.append(comment_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-41e321484b89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#FastText.load_model('cc.ru.300')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec-ruscorpora-300\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#w2v_file = codecs.open('cc.ru.300.bin.gz', encoding='utf-8')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\gensim-data\\word2vec-ruscorpora-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-ruscorpora-300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-ruscorpora-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[1;34m(word, weights)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "from gensim.models.wrappers import FastText\n",
    "import gensim.downloader as api\n",
    "\n",
    "#FastText.load_model('cc.ru.300')\n",
    "model = api.load(\"word2vec-ruscorpora-300\")\n",
    "\n",
    "#w2v_file = codecs.open('cc.ru.300.bin.gz', encoding='utf-8')\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)  # or binary=False if the model is not compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Н'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-381c2724eedc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# 6. Preprocess class labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mis\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Н'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_list = list()\n",
    "for intent in data_df['Intent']:\n",
    "    label_list.append(intent)\n",
    "\n",
    "# 4. Load pre-shuffled MNIST data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_data, label_list, test_size=0.2, random_state = 42, stratify = label_list)\n",
    "\n",
    " \n",
    "# 5. Preprocess input data\n",
    "#X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 25)\n",
    "Y_test = np_utils.to_categorical(y_test, 25)\n",
    " \n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(25, activation='softmax'))\n",
    "          \n",
    "\n",
    " \n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "          \n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, nb_epoch=10, verbose=1)\n",
    " \n",
    "# 10. Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags = list()\n",
    "#for word in model.vocab:\n",
    "    #print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ага\n",
      "ага_INTJ\n",
      "держи\n",
      "держать_INFN\n",
      "карман\n",
      "карман_NOUN\n",
      "шире\n",
      "шире_NOUN\n",
      "['ага_INTJ', 'держать_INFN', 'карман_NOUN', 'шире_NOUN']\n",
      "ага\n",
      "ага_INTJ\n",
      "держи\n",
      "держать_INFN\n",
      "карман\n",
      "карман_NOUN\n",
      "шире\n",
      "шире_NOUN\n",
      "['ага', 'держи', 'карман', 'шире']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['ага','держи','карман','шире']\n",
    "for i in range(len(tokens)):\n",
    "    print(tokens[i])\n",
    "    tokens[i] = morph.parse(tokens[i])[0].normal_form\n",
    "    t = morph.parse(tokens[i])[0]\n",
    "    tokens[i] = tokens[i] + '_' + str(t.tag.POS)\n",
    "    print(tokens[i])\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = ['ага','держи','карман','шире']\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    token = morph.parse(token)[0].normal_form\n",
    "    t = morph.parse(token)[0]\n",
    "    token = token+'_'+str(t.tag.POS)\n",
    "    print(token)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "word = 'дома'\n",
    "word = morph.parse(word)[0].normal_form\n",
    "t = morph.parse(word)[0]\n",
    "word = word+'_'+str(t.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3661"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ц', 'Н', 'Ш', 'Ж', 'И', 'Ч', 'Т', 'Щ', 'А', 'Г', 'У', 'Х', 'Б',\n",
       "       'Д', 'Л', 'В', 'Е', 'П', 'К', 'С', 'М', 'Ф', 'З', 'Р', 'О'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Tamir\\AppData\\Local\\pip\\Cache\\wheels\\9b\\04\\dd\\7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: dawg-python, docopt, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n",
      "distributed 1.20.2 requires msgpack-python, which is not installed.\n",
      "tensorflow 1.11.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.4.3 which is incompatible.\n",
      "pyowm 2.8.0 has requirement requests<2.19,>=2.18.2, but you'll have requests 2.19.1 which is incompatible.\n",
      "copydoc 1.0.9 has requirement beautifulsoup4==4.4.1, but you'll have beautifulsoup4 4.6.3 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
