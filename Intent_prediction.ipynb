{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(columns=['text','Intent'])\n",
    "filenames = list()\n",
    "intervention_subgraphs = list()\n",
    "for directory, subdirectories, files in os.walk('C:/Users/Tamir/Desktop/HSE_Project/excel_files/'):\n",
    "    for file in files:\n",
    "        filenames.append(os.path.join(directory,file))\n",
    "for filename in filenames:\n",
    "    if (filename[len(filename) - 22] == '~'):\n",
    "        continue\n",
    "    filename = filename.replace(\" \", \" \")\n",
    "    post_df = pd.read_excel(filename)\n",
    "    \n",
    "        #fix to add columns if they are not in the xlsx file\n",
    "    if 'attachments' not in post_df.columns:\n",
    "        post_df['attachments'] = ''\n",
    "    if 'reply_to_comment' not in post_df.columns:\n",
    "        post_df['reply_to_comment'] = 0\n",
    "    if 'reply_to_user' not in post_df.columns:\n",
    "        post_df['reply_to_user'] = 0\n",
    "        \n",
    "    flex_df = post_df.drop(columns=['attachments','date','from_id','id','reply_to_comment','reply_to_user','Content','Intervention'])\n",
    "    data_df = data_df.append(flex_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change in 3158\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Removes stop words from the given text\n",
    "    :param text: string with the text\n",
    "    :param remove_stopwords: flag to enable removal of stop words\n",
    "    :return: set of tokens without any stop words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('russian'))\n",
    "        tokens = [w for w in tokens if w not in stops]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "delete_index = list()\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    if ( pd.isnull(data_df['text'].loc[index]) or pd.isnull(data_df['Intent'].loc[index]) \n",
    "              or (data_df['Intent'].loc[index] == ' -' ) ):\n",
    "        delete_index.append(index)        \n",
    "    else:\n",
    "        data_df['text'].loc[index] = clean_text(data_df['text'].loc[index])\n",
    "\n",
    "data_df = data_df.drop(index = delete_index)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "\n",
    "for index,row in data_df.iterrows():\n",
    "    data_df['Intent'].loc[index] = data_df['Intent'].loc[index][0]\n",
    "    if(data_df['Intent'].loc[index] == 'E'):\n",
    "        print('change in ' + str(index))\n",
    "        data_df['Intent'].loc[index] = 'Е'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "documents = [TaggedDocument(data_df['text'].loc[i], data_df['Intent'].loc[i]) for i in range(len(data_df))]\n",
    "model = Doc2Vec(documents,dm=1, vector_size=10, window=4, min_count=1,epochs=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Anaconda\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('русских', 0.9985089898109436),\n",
       " ('которые', 0.9984912872314453),\n",
       " ('лет', 0.9984446167945862),\n",
       " ('хотя', 0.99834805727005),\n",
       " ('нато', 0.9983335137367249),\n",
       " ('сергей', 0.9982507228851318),\n",
       " ('войны', 0.9979708194732666),\n",
       " ('вообще', 0.9979184865951538),\n",
       " ('тебе', 0.9978156089782715),\n",
       " ('будут', 0.9978112578392029)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('путин')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ц', 'Н', 'Ш', 'Ж', 'И', 'Ч', 'Т', 'Щ', 'А', 'Г', 'У', 'Х', 'Б',\n",
       "       'Д', 'Л', 'В', 'Е', 'П', 'К', 'С', 'М', 'Ф', 'З', 'Р', 'О'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('У', 0.7306615114212036),\n",
       " ('А', 0.7306163311004639),\n",
       " ('Г', 0.7295863032341003),\n",
       " ('Ж', 0.7294228672981262),\n",
       " ('Б', 0.7290605306625366),\n",
       " ('Ц', 0.7276159524917603),\n",
       " ('В', 0.7276017665863037),\n",
       " ('С', 0.7266857624053955),\n",
       " ('М', 0.7257146835327148),\n",
       " ('Х', 0.7248767018318176)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"ага держи карман шире\".split()\n",
    "\n",
    "new_vector = model.infer_vector(tokens)\n",
    "sims = model.docvecs.most_similar([new_vector])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
